---
title: "Internship Project"
author: "Anastasiia Deviataieva (Student Number: 24100519)"
format:
  pdf:
    geometry: 
      - top=1in        
      - bottom=1in     
      - left=1in       
      - right=1in   
    extra_dependencies: ["fvextra"]
header-includes:
  - \usepackage{fvextra}  # Using fvextra to control output
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}} #for chuncks 
editor: visual
execute:
  error: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, tidy = TRUE, tidy.opts = list(width.cutoff = 80)) #Code width
options(width = 80) #Code output width
```

# **Decomposition of somatic mutation profiles into mutational signatures**

## Installing packages

```{r, message=FALSE}
# ============================== BLOCK 0 ===================================
# PACKAGE INSTALLATION & LOADING
# ==========================================================================
# install_if_missing()
# - Installs a vector of packages from CRAN or Bioconductor, only if they’re
#   not already available in the current R library.
# - bioc = TRUE switches to Bioconductor installation via BiocManager.
# - Uses a fixed CRAN mirror to avoid interactive prompts on some systems.
# ==========================================================================
install_if_missing <- function(pkgs, bioc = FALSE) {
  if (bioc) {
    # Ensure BiocManager itself is available for Bioconductor installs
    if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager", repos = "https://cloud.r-project.org")
    # Install each missing Bioconductor package without asking to update others
    for (p in pkgs) if (!requireNamespace(p, quietly = TRUE)) BiocManager::install(p, ask = FALSE, update = FALSE)
  } else {
    # Install each missing CRAN package, pulling suggested deps as needed
    for (p in pkgs) if (!requireNamespace(p, quietly = TRUE)) install.packages(p, repos = "https://cloud.r-project.org", dependencies = TRUE)
  }
}

# ----------------------------- CRAN -----------------------------------
# sigminer     : signature extraction/visualization
# pheatmap     : quick heatmaps
# ggplot2      : general-purpose plotting
# reshape2     : melt/cast utilities for data reshaping
# scales       : axis scaling and palettes
# clue         : clustering utilities
# NMF          : non-negative matrix factorization
# dynamicTreeCut: dynamic dendrogram pruning
# readxl       : Excel import
# dplyr        : data manipulation (select, filter, mutate, etc.)
# tidyr        : data tidying (pivot, separate, unite)
# survival     : survival analysis (Cox model, KM curves)
# survminer    : visualization for survival models
# dendextend   : dendrogram manipulation/plotting
# RColorBrewer : color palettes
# circlize     : circular visualizations
# cluster      : clustering algorithms
# ggplotify    : convert non-ggplot objects into ggplot objects
install_if_missing(c(
  "sigminer","pheatmap","ggplot2","reshape2","scales","clue","NMF","dynamicTreeCut","readxl","dplyr",
  "tidyr","survival","survminer","dendextend","RColorBrewer","circlize","cluster", "ggplotify"
), bioc = FALSE)

# --------------------------- Bioconductor ------------------------------
# maftools                   : MAF handling, oncoplots, Ti/Tv, etc.
# BSgenome.Hsapiens.UCSC.hg19: hg19 reference for context extraction
# SomaticSignatures          : mutation contexts (e.g., SBS-96) + normalization by trinucleotide opportunity
# VariantAnnotation          : VCF/annotation utilities used by several genomics tools
# IRanges                    : core range operations used across Bioc packages
# org.Hs.eg.db               : gene annotations (Entrez, symbols, etc.)
# AnnotationDbi              : database interface for annotations
# ComplexHeatmap             : advanced heatmaps + annotations
install_if_missing(c(
  "maftools","BSgenome.Hsapiens.UCSC.hg19","SomaticSignatures","VariantAnnotation",
  "IRanges","org.Hs.eg.db","AnnotationDbi","ComplexHeatmap"
), bioc = TRUE)

# --------------------------- Load libraries ----------------------------
# Suppress verbose startup messages so the console/logs remain clean.
suppressPackageStartupMessages({
  library(maftools)
  library(sigminer)
  library(pheatmap)
  library(ComplexHeatmap)
  library(BSgenome.Hsapiens.UCSC.hg19)
  library(SomaticSignatures)
  library(VariantAnnotation)
  library(IRanges)
  library(ggplot2)
  library(reshape2)
  library(readxl)
  library(org.Hs.eg.db)
  library(AnnotationDbi)
  library(scales)
  library(clue)
  library(NMF)
  library(dynamicTreeCut)
  library(cluster)
  library(dplyr)
  library(tidyr)
  library(survival)
  library(survminer)
  library(dendextend)
  library(RColorBrewer)
  library(circlize)
  library(ggplotify) 
})

```

## Data loading

```{r}
# ============================== BLOCK 1 =================================
# DATA LOADING 
# ========================================================================

# ---------- OUTPUT FOLDER ----------
# Create the output directory if it doesn’t exist.
# - path.expand() resolves "~" to a full path.
# - recursive = TRUE creates any missing parent folders.
out_dir <- "~/Desktop/Internship_outputs_2" # Directory where all outputs will be written
dir.create(path.expand(out_dir), showWarnings = FALSE, recursive = TRUE)

# ---------- DATA LOADING ----------
# Data source: Excel workbook with a MAF-like table on sheet 4
excel_path  <- "/Users/elizaavveettaaa/Downloads/Internship/011.xlsx"
excel_sheet <- 4  # 4th sheet contains the MAF table

message("[INFO] Reading Excel sheet ", excel_sheet, ": ", excel_path)
# Read the sheet while PRESERVING the raw cell types per cell
df_xl <- read_excel(
  path.expand(excel_path),
  sheet     = excel_sheet, 
  skip      = 2,           # Skip two non-data rows at the top
  col_names = TRUE,        # Use the next row as column 
  col_types = "list"       # Preserve raw, per-cell types (no coercion).
) %>% as.data.frame(stringsAsFactors = FALSE) # Keep character columns as character, not factors.
```

## Data recovery

Excel loves to “help” by auto-converting cell values that look like dates (e.g., 3/8, 01-02) into actual Date values. That’s disastrous for genomics tables: strings such as 3/8 might be ratios or coordinates, not calendar dates. When those cells reach R as `Date/POSIX*`, downstream tools (e.g., maftools) will either fail or silently coerce them to `NA`. This block detects which columns and cells were auto-converted to Date/POSIX by Excel and collects them so you can inspect exactly what got corrupted and restore the original strings (in Block 1 you already read with `col_types="list"` to preserve per-cell types; here you enumerate the cells that still came through as Date/POSIX).

```{r}
# ============================== BLOCK 2.1 (OPTIONAL) =================================
# USEFUL BLOCK 

# Some cells were incorrectly converted to dates by Excel. This block finds those cells
# and collects them for inspection so we can repair them reliably downstream
# =====================================================================================

# # Detect columns that contain at least one cell of class Date or POSIX
# # (i.e., values that Excel likely auto-converted).
# date_cols <- names(df_xl)[sapply(df_xl, function(col) {
#   any(sapply(col, inherits, what = c("Date", "POSIXct", "POSIXt"))) })]
# 
# # Show the list of column names where Excel has converted values to dates
# print(date_cols)
# 
# # Prepare an empty collector for all misinterpreted cells (“freaky” values).
# # Columns: the column name, the (1-based) row index, and the date value as character.
# bad_vals <- data.frame(column = character(), row = integer(), value = character(), stringsAsFactors = FALSE)
# 
# # Loop over each problematic column to extract the offending cells
# for (nm in date_cols) {
#    # Indices of rows where the cell in this column is a Date/POSIX*
#   idx <- which(sapply(df_xl[[nm]], inherits, what = c("Date", "POSIXct", "POSIXt")))
#   if (length(idx) > 0) {
#     # Convert the date values to character strings for readability
#     vals <- sapply(df_xl[[nm]][idx], function(x) as.character(x))
#     # Bind the results to the bad_vals data frame
#     # We add 2 to the row index because read_excel() skipped two header rows
#     bad_vals <- rbind(
#       bad_vals,
#       data.frame(column = nm, row = idx + 2, value = vals, stringsAsFactors = FALSE))
#   }
# }
# View(bad_vals)
```

**In the BLOCK 2.2 output** you will see “`‘select()’ returned many:1 mapping between keys and columns`” which is produced by the `AnnotationDbi` package when calling the `select()` function. It means that for some of your Entrez identifiers in the selected annotation database (`org.Hs.eg.db)` several records were found in the `SYMBOL` column. In other words, the relationship between `ENTREZID` and `SYMBOL` is not always “one-to-one”: for one Entrez ID there may exist several synonymous names in the database, therefore `select()` returns several rows for one key. The parameter `multiVals = "first"` forces selecting only the first match, but the message serves as a reminder that ambiguous correspondences exist in the annotation database.

```{r}
# ============================== BLOCK 2.2 =================================
# RESTORE CORRECT VALUES for Hugo_Symbol, SYMBOL columns
# ========================================================================
restore_hugo_symbols <- function(df, orgdb = org.Hs.eg.db) {
  # Find rows where Hugo_Symbol is clearly wrong:
  # - cell is a Date/POSIX (Excel auto-conversion case),
  # - or missing (NA),
  # - or an empty string
  bad_idx <- which(
    sapply(df$Hugo_Symbol, inherits, what = c("Date","POSIXct","POSIXt")) |
    is.na(df$Hugo_Symbol) |
    df$Hugo_Symbol == "")

  if (length(bad_idx) == 0L) return(df) # If nothing to fix, return the data frame as is

  # Extract the Entrez IDs for the bad rows and coerce to character
  # (AnnotationDbi::select expects keys as character)
  entrez_ids <- as.character(df$Entrez_Gene_Id[bad_idx])

  # Query the organism annotation package (org.Hs.eg.db) for SYMBOLs that correspond to those ENTREZ IDs
  mapping <- AnnotationDbi::select(
    orgdb, 
    keys = entrez_ids,  
    keytype= "ENTREZID", # we are matching by Entrez Gene ID
    columns= "SYMBOL",   # we want the gene symbol back
    multiVals = "first") # if an ENTREZID maps to multiple symbols, take the first

  # We compare the found names in the order as in the table
  matched <- mapping$SYMBOL[match(entrez_ids, mapping$ENTREZID)]
  
  # Overwrite only the "bad" cells with the mapped SYMBOLs.
  # Rows that failed to map will get NA here (better than a wrong date)
  df$Hugo_Symbol[bad_idx] <- matched   
  df$SYMBOL[bad_idx] <- matched

  return(df) # Return the updated table.
}

df_xl <- restore_hugo_symbols(df_xl)
```

The display format “`95507-09-28`” or “`216755-06-17`” is a consequence of the fact that Excel considered the number in the cell a “date” and applied the yyyy-mm-dd format to it. Inside Excel, a date is just an integer equal to the number of days that have passed since `1899-12-30`; for example:\
`34 189 526` → format as date → `95 507-09-28`.\
`78 474 346` → format as date → `216 755-06-17`.\
`131 310 650` → format as date → `361 416-05-06`.\
To restore the original coordinates from such strings, you need to perform the inverse operation: parse the string into year-month-day and compute how many days have passed between this date and `1899-12-30`. Below is an example of a function for `Start_Position` and `End_Position` that uses the well-known algorithm for converting a calendar date to a number of days (Hinnant algorithm). It correctly handles both “normal” numbers, and date-strings, and real Date/POSIXct class objects, including years \> 9999.

```{r}
# ============================== BLOCK 2.3 =================================
# RESTORE CORRECT VALUES for Start_Position, End_Position columns
# ==========================================================================
# --- Convert a civil date (year, month, day) to an absolute day number ---
# Implementation follows the well-known algorithm by H. Hinnant. We treat Jan/Feb
# as months 13/14 of the previous year so leap-year math works uniformly
days_from_civil <- function(y, m, d) {
  # Map Jan/Feb to months 13/14 of the previous year
  if (m <= 2) {
    y <- y - 1
    m <- m + 12
  }
  era <- y %/% 400
  yoe <- y - era * 400                             # year-of-era
  doy <- (153 * (m - 3) + 2) %/% 5 + d - 1         # day-of-year in the shifted calendar
  doe <- yoe * 365 + yoe %/% 4 - yoe %/% 100 + doy
  era * 146097 + doe                               # 146097 = 365*400 + 97 leap days
}

# Precompute the absolute day number for Excel's origin (1899-12-30)
# Note: using 1899-12-30 matches Excel's historical 1900-leap-year quirk
base_days <- days_from_civil(1899, 12, 30)

# Convert a string like "95507-09-28" (or any YYYY-MM-DD) into an Excel-style
# serial day number by subtracting the base (1899-12-30)
date_to_serial <- function(date_str) {
  parts <- strsplit(date_str, "-")[[1]]
  y <- as.numeric(parts[1]); m <- as.numeric(parts[2]); d <- as.numeric(parts[3])
  days_current <- days_from_civil(y, m, d)
  days_current - base_days}

# Main fixer for coordinate-like columns (Start_Position / End_Position).
fix_position_column <- function(col) {
  sapply(col, function(x) {
    # Empty/missing cell → NA
    if (length(x) == 0 || (length(x) == 1 && is.na(x))) {return(NA_real_)}
    
    # Already numeric (integer/double/logical) → coerce to numeric and return
    if (is.numeric(x)) {return(as.numeric(x))}
    
    # Date/POSIX → convert to serial days (Excel origin)
    if (inherits(x, c("Date", "POSIXct", "POSIXt"))) {
      y <- as.numeric(format(as.Date(x), "%Y"))
      if (y <= 9999) {
        # Typical case: safe to subtract the Excel origin directly
        return(as.numeric(as.Date(x) - as.Date("1899-12-30")))
      } else {
        # Pathological large-year case: format and use robust converter
        date_str <- format(x, "%Y-%m-%d")
        return(date_to_serial(date_str))
      }
    }
    # Character: handle ISO-like dates first, then fallback to digits-only
    val <- as.character(x)
    
    if (grepl("^\\d{4,}-\\d{2}-\\d{2}$", val)) {
      return(date_to_serial(val))
    } else {
      # Keep only digits to preserve pure integer coordinates like "34,189,526"
      num <- gsub("[^0-9]", "", val)
      if (nchar(num) > 0) {
        return(as.numeric(num))
      } else {
        return(NA_real_)
      }
    }
  })
}

# Apply to coordinate columns. Output type is numeric (double) with NA where unresolved
df_xl$Start_Position <- fix_position_column(df_xl$Start_Position)
df_xl$End_Position   <- fix_position_column(df_xl$End_Position)
```

```{r}
# ============================== BLOCK 2.4 =================================
# RESTORE CORRECT VALUES for Exon_Number, EXON, INTRON, cDNA_position, 
# CDS_position, Protein_position columns
# ==========================================================================
# Fix Excel "auto-date" corruption in specific columns with per-column rules.
# RULES (by column group):
#   - "auto_dm_or_myy": choose between  "dd/mm"  and  "mm/yy"  using a concrete heuristic:
#       * compute two-digit year (yy) from the Date
#       * if yy > 12  → format "mm/yy"
#         else        → format "dd/mm"
#   - "m_yyyy": always format as "m/YYYY"
#
# Note: The “yy > 12” heuristic uses only the two-digit year; edge cases (yy in 00..12)
# are forced to "dd/mm" 

fix_mixed_date_columns <- function(df,
  rules = list(
    auto_dm_or_myy = c("Exon_Number", "EXON", "INTRON"),
    m_yyyy         = c("cDNA_position", "CDS_position", "Protein_position")),
  zero_pad = FALSE) {  # if TRUE, pad day/month in "dd/mm" and "mm/yy" with leading zeros
  
  # Treat empty list-elements and explicit NAs as missing
  is_na_cell <- function(e) length(e) == 0 || (length(e) == 1 && is.na(e))
  
  # Formatter 1:
  #   If two-digit year (yy) > 12 → "mm/yy", else "dd/mm"
  #   Optionally zero-pad day/month for "dd/mm" and "mm/yy"
  fmt_auto_dm_or_myy <- function(e) {
    d  <- as.Date(e)
    dd <- as.integer(format(d, "%d"))
    mm <- as.integer(format(d, "%m"))
    yy2 <- as.integer(format(d, "%y"))   # two-digit year
    dm <- if (zero_pad) sprintf("%02d/%02d", dd, mm) else paste0(dd, "/", mm)
    my <- if (zero_pad) sprintf("%02d/%02d", mm, yy2) else paste0(mm, "/", yy2)
    if (yy2 > 12) my else dm}
  
  # Formatter 2:
  #   Always "m/YYYY" (month without leading zero / full four-digit year)
  fmt_m_yyyy <- function(e) {
    d  <- as.Date(e)
    mm <- as.integer(format(d, "%m"))
    yy4 <- format(d, "%Y")
    paste0(mm, "/", yy4)}

  # Apply a chosen formatter to a single column
  apply_formatter_to_col <- function(x, formatter) {
    # If the column has no Date/POSIX cells, return it unchanged
    has_date <- any(sapply(x, inherits, what = c("Date", "POSIXct", "POSIXt")))
    if (!has_date) return(x)

    # Build a character vector, converting only date-like cells
    vapply(seq_along(x), function(i) {
      e <- x[[i]]
      if (is_na_cell(e)) return(NA_character_)
      if (inherits(e, c("Date","POSIXct","POSIXt"))) {
        switch(formatter,
          auto_dm_or_myy = fmt_auto_dm_or_myy(e),
          m_yyyy         = fmt_m_yyyy(e),
          stop("Unknown formatter: ", formatter)
        )
      } else { # Column contains dates → coerce non-date cells to character for consistency
        as.character(e)
      }
    }, character(1)) 
  }

  # Walk through rule groups and patch the requested columns
  for (fmt in names(rules)) {
    cols <- rules[[fmt]]
    for (nm in cols) {
      if (nm %in% names(df)) {
        df[[nm]] <- apply_formatter_to_col(df[[nm]], formatter = fmt)
      }
    }
  }
  df
}

df_xl <- fix_mixed_date_columns(df_xl)
```

```{r}
# ============================== BLOCK 2.5 =================================
# RESTORE CORRECT VALUES for ExAC_AF_EAS, ExAC_AF_OTH, ExAC_AC_AN_Adj, 
# ExAC_AC_AN, ExAC_AC_AN_AFR, ExAC_AC_AN_AMR, ExAC_AC_AN_EAS, ExAC_AC_AN_FIN, 
# ExAC_AC_AN_NFE, ExAC_AC_AN_OTH, ExAC_AC_AN_SAS columns

# Replace all Excel auto-converted Date/POSIX cells with NA (since the remaining columns already contain a lot of NA and we do not need them for further analysis)
# ==============================================================================
# Find columns that have at least one Date/POSIX cell
date_cols <- names(df_xl)[sapply(df_xl, function(col) {
  any(sapply(col, inherits, what = c("Date", "POSIXct", "POSIXt")))})]

# Collect samples of freaky cells (for inspection/debug)
bad_vals <- data.frame(column = character(), row = integer(), value = character(), stringsAsFactors = FALSE)

# Replace all Date/POSIX cells with NA (list-safe)
for (nm in date_cols) {
  # indices of Date/POSIX cells in this column
  idx <- which(sapply(df_xl[[nm]], inherits, what = c("Date", "POSIXct", "POSIXt")))
  if (length(idx) == 0) next

  # add to samples (row index +2 because read_excel() used skip = 2)
  vals <- vapply(df_xl[[nm]][idx], function(x) as.character(x), character(1))
  bad_vals <- rbind(bad_vals, data.frame(column = nm, row = idx + 2, value = vals, stringsAsFactors = FALSE))

  # replace with NA; handle list vs atomic columns
  if (is.list(df_xl[[nm]])) {
    df_xl[[nm]][idx] <- replicate(length(idx), NA, simplify = FALSE)
  } else {
    df_xl[[nm]][idx] <- NA
  }
  # message(sprintf("[FIX] Column '%s': %d date-cells -> NA", nm, length(idx)))
}

# Replace literal "." with NA in list and atomic columns
for (nm in names(df_xl)) {
  if (is.list(df_xl[[nm]])) {
    df_xl[[nm]] <- lapply(df_xl[[nm]], function(e) if (is.character(e) && identical(e, ".")) NA else e)
  } else {
    df_xl[[nm]][df_xl[[nm]] == "."] <- NA
  }
}

```

```{r}
# ============================== BLOCK 2.6 ================================
# Materialize the resulting data.frame to TSV for downstream maftools::read.maf()
# =========================================================================

# Identify columns that are still list-columns (after readxl col_types="list")
lst <- vapply(df_xl, is.list, logical(1))

# Flatten list-columns:
# - empty elements or explicit NA -> NA
# - if a cell has multiple elements, keep ONLY the first (e[[1]])
# - R will infer the atomic type of the resulting column from the extracted values
df_xl[lst] <- lapply(df_xl[lst], function(col)
  sapply(col, function(e) if (length(e) == 0 || (length(e) == 1 && is.na(e))) NA else e[[1]],
         simplify = "vector"))

# ---------- SAVE RESTORED DATA ----------
# Write the cleaned table to a TSV
maf_path <- file.path(path.expand(out_dir), "maf_from_excel_sheet4.tsv")
write.table(df_xl, maf_path, sep = "\t", quote = FALSE, row.names = FALSE)
```

## Data pre-processing and QC

```{r}
# ============================== BLOCK 3.1 ================================
# READ MAF
# ======================================================================

# ---------- READ MAF ----------
# Validate input path and read the MAF into a maftools MAF object.
#    - path.expand() allows '~' in paths.
#    - read.maf() parses common MAF columns and stores them in @data.
if (!file.exists(path.expand(maf_path))) stop("File not found: ", maf_path)
message("[INFO] Reading MAF: ", maf_path)
maf <- maftools::read.maf(maf_path)

# Plotting MAF summary
pdf(file.path(out_dir, "Maf_summary.pdf"), width = 10, height = 6)
maftools::plotmafSummary(maf = maf, addStat = "median", dashboard = TRUE)
dev.off()

# Plotting oncoplot
pdf(file.path(out_dir, "oncoplot_top25.pdf"), width = 9.33, height = 6)
maftools::oncoplot(maf = maf, top = 25)
dev.off()


# ============================ BLOCK 3.2 ===============================
# CLEAN & PREPROCESS 
# Keep only SNPs → Keep only "PASS" in FILTER → sample cutoff (SNV ≥ 15)
# ======================================================================

maf_snv <- maftools::subsetMaf(maf = maf, query = 'Variant_Type == "SNP"') 
maf_filt <- maftools::subsetMaf(maf = maf_snv, query = 'FILTER == "PASS"')

# If you don't want to apply sample cutoff (SNV ≥ 15), uncomment on the next line and comment the rest of the lines of this block
# maf_clean <- maf_filt  
# Per-sample SNV counts and a stability cutoff: keep samples with ≥ 15 SNVs → improves NMF robustness for signature extraction
snv_counts <- table(maf_filt@data$Tumor_Sample_Barcode)
keep_tsb   <- names(snv_counts)[snv_counts >= 15]
maf_clean  <- maftools::subsetMaf(maf = maf_filt, tsb = keep_tsb)

# Progress message: number of retained samples and total SNVs after cleaning
message("[INFO] After preprocessing: samples ", length(keep_tsb), "; total SNVs ", nrow(maf_clean@data))
```

```{r}
# ============================ BLOCK 3.3 ===============================
# QC snapshots with maftools (Ti/Tv, Rainfall)
# ======================================================================
# Rationale: fast visual checks to spot sample outliers (Ti/Tv) and localized hypermutation/kataegis (rainfall) before downstream signature extraction.

# Ti/Tv per sample
pdf(file.path(out_dir, "QC_TiTv_plot.pdf"), width = 7, height = 5)
maftools::plotTiTv(maftools::titv(maf = maf_clean, plot = FALSE, useSyn = TRUE))
dev.off()

# Rainfall — pick up to 3 samples with the highest SNP counts and render one page per sample
snv_per_sample <- table(maf_clean@data$Tumor_Sample_Barcode[maf_clean@data$Variant_Type == "SNP"])
samples_to_plot <- names(head(sort(snv_per_sample[snv_per_sample > 0], decreasing = TRUE), 3))
if (length(samples_to_plot) > 0) {
  pdf(file.path(out_dir, "QC_Rainfall_plots.pdf"), width = 9, height = 6)
  for (s in samples_to_plot) {
    p <- try(maftools::rainfallPlot(maf = maf_clean, tsb = s), silent = TRUE) # some versions draw directly
    if (inherits(p, "ggplot")) print(p) # print if a ggplot object is returned
  }
  dev.off()
}


# ============================ BLOCK 3.4 ===============================
# PLOT: SNP distribution per sample
# ======================================================================
# Build per-sample SNP counts from the cleaned MAF.
cnt_df <- as.data.frame(table(maf_clean@data$Tumor_Sample_Barcode), stringsAsFactors = FALSE)
names(cnt_df) <- c("Sample", "SNPs")

# Order samples by count so bars appear sorted on the chart
cnt_df$Sample <- factor(cnt_df$Sample, levels = cnt_df$Sample[order(cnt_df$SNPs, decreasing = FALSE)])

# --------------------------- BAR CHART ---------------------------------
p_all <- ggplot(cnt_df, aes(x = Sample, y = SNPs)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +  
  # Axis tuning
  scale_y_continuous(limits = c(0, max(cnt_df$SNPs, na.rm = TRUE)),
                     expand = expansion(mult = c(0, 0.02)),
                     breaks = seq(0, max(cnt_df$SNPs), by = 50)) +
  labs(title = "SNP counts per sample",
       x = "Sample",
       y = "Number of SNPs") +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(), # drop grid
    panel.grid.major.x = element_line(),  # keep grid
    axis.text.y = element_text(size = 8))

ggsave(filename = file.path(path.expand(out_dir), "SNP_per_sample_all.pdf"),
       plot = p_all, width = 9, height = 10, dpi = 300)

# --------------------------- HISTOGRAM ----------------------------------
p_hist <- ggplot(cnt_df, aes(x = SNPs)) +
  geom_histogram(bins = 30, fill = "blue") +
  coord_flip() +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.02))) +
  labs(title = "Distribution of SNP counts across samples",
       x = "SNPs per sample",
       y = "Number of samples") +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.major.x = element_line())

ggsave(filename = file.path(path.expand(out_dir), "SNP_counts_histogram.pdf"),
       plot = p_hist, width = 8, height = 6, dpi = 300)


message(" Samples: ", nrow(cnt_df),
        " | Total SNPs: ", sum(cnt_df$SNPs), 
        " | Median per sample: ", median(cnt_df$SNPs))
```

## Prepare 96-Context Matrices

Goal of this block:

-   1\) Build the 96-channel (SBS-96) mutation count matrix for NMF-based signature extraction and COSMIC matching using sigminer::sig_tally().

-   2\) Build SBS-96 with SomaticSignatures and choose ONE of two normalizations:

    -   \(a\) per-sample frequencies (no opportunity correction) \[better with COSMIC\]

    -   \(b\) opportunity-normalized (adjust for 3-mer abundance in hg19) \[closest to the article's\]

**Recommendation**: use Normalization 1 (conversion of each column into fractions without an opportunity correction), because it preserves the relative shape of the 96-channel spectrum on the same scale in which COSMIC signatures are specified (distributions with sum = 1). With normalization 2 we additionally divide by contexts by the frequencies of 3-mers from the reference set; this changes the ratios between channels and especially relabels rare contexts. Also, due to the use of “genomic” norms, the profile can additionally be shifted because of differences in the composition of triplets, and the cosine similarity with COSMIC decreases. Moreover, estimates via kmerFrequency can introduce variability, and dividing by very rare triplets amplifies noise with a small number of mutations. As a result, the extracted signatures turn out to be farther from the COSMIC standards. If the priority is stable matching with COSMIC, use normalization 1.

```{r}
# ============================== BLOCK 4 ================================
# SBS-96 CONTEXTS FROM MAF (two complementary routes)
# ======================================================================

# ---------- SBS-96 FROM MAF -------------------------------------------
ref_pkg <- "BSgenome.Hsapiens.UCSC.hg19"
message("[INFO] Building SBS-96 contexts with ref=", ref_pkg)
mt <- sigminer::sig_tally(
  maf_clean,
  ref_genome = ref_pkg,
  mode = "SBS",
  use_syn = TRUE)   # set to FALSE if Variant_Classification is missing or unreliable

# Save the *raw* 96-context matrix for NMF downstream
# Note: many sigminer functions accept this orientation directly.
write.csv(mt$nmf_matrix, file.path(out_dir, "mut_matrix_96.csv"))  # samples × 96

# Build SBS-96 via SomaticSignatures for opportunity-normalized profiles
vr <- with(maf_clean@data, VRanges(
  seqnames    = Chromosome,
  ranges      = IRanges(start = as.integer(Start_Position), width = 1L),
  ref         = Reference_Allele,
  alt         = Tumor_Seq_Allele2,
  sampleNames = Tumor_Sample_Barcode))
# Extract the 3-mer sequence context surrounding each SNV from hg19
ctx <- mutationContext(vr, ref = BSgenome.Hsapiens.UCSC.hg19)


# --- NORMALISAION 1 (works best for camparing with COSMIC) ---------------------------------
# The counts are transformed to frequencies, such that the sum of frequencies of each group equal 1
mm_norm <- motifMatrix(ctx, group = "sampleNames", normalize = TRUE) # 96 × samples


# # --- NORMALISAION 2 (closest to the article's, bad with COSMIC comparing) ------------------
# # Opportunity-normalisation
# # Build a raw counts matrix first (no per-sample scaling)
# mm_counts <- motifMatrix(ctx, group = "sampleNames", normalize = FALSE)
# 
# # Compute the expected frequency of each 3‑mer in the hg19 reference genome.
# # These frequencies represent the 'opportunities' for mutations and are used as denominators
# # when normalizing counts, matching the method described in the article.
# norms <- SomaticSignatures::kmerFrequency(BSgenome.Hsapiens.UCSC.hg19, k = 3)
# norms <- norms / sum(norms)  # convert counts to probabilities
# # Normalize each count by the corresponding 3‑mer frequency.
# mm_norm <- SomaticSignatures::normalizeMotifs(mm_counts, norms)
# 
# # Finally, convert to per-sample frequencies (each column to sum 1) for heatmaps/clustering
# mm_norm <- sweep(mm_norm, 2, pmax(colSums(mm_norm), 1e-12), "/")


# --- Save the normalized matrix ------------------------------------------------------------
write.csv(as.matrix(mm_norm), file.path(out_dir, "mut_matrix_96_normalized_hg19.csv"))

```

## Normalization & SBS-96 Alignment

This block is needed only in the case that you decided to use normalized 96-profiles (`mm_norm`) for decomposition and/or comparison with COSMIC. This will take place in the sigminer ecosystem, where the SBS format is expected (names of the type A\[C\>T\]A).

Block 4.1 automatically matches the rows of `mm_norm` to the columns of `mt$nmf_matrix` (via correlations and the Wenger algorithm) and rearranges `mm_norm` into SBS order so that it can be safely fed into `sigminer`. If you will not use normalized profiles for NMF/matching (and will go directly from `mt$nmf_matrix`), leave block 4.1 commented out—it is not needed.

```{r}
# ============================== BLOCK 4.1 ================================
# Normalization & SBS-96 Alignment
# =========================================================================
# This block reconciles two different 96-channel catalogues of the same samples:
#   (1) mm_norm (SomaticSignatures):   96 x S, opportunity-normalized profiles with
#       *row names* in a SomaticSignatures-specific format (e.g., "CA A.A").
#   (2) mt$nmf_matrix (sigminer):      S x 96, raw counts with *column names* in SBS style
#       "A[C>A]A".

# Because the two toolchains label/format the 96 contexts differently, we build a
# *data-driven mapping* between the 96 rows of mm_norm and the 96 columns of mt$nmf_matrix.
# We do this by:
#   - normalizing both catalogues column-wise to proportions,
#   - computing a 96x96 Pearson correlation between "rows of mm_norm" and "columns of mt",
#   - solving an assignment (Hungarian / LSAP) to find a 1-to-1 pairing that maximizes
#     total similarity,
#   - REORDERING mm_norm’s rows into SBS order (i.e., the column order of mt$nmf_matrix).

# TRADE-OFFS / CAVEATS
# - The mapping is *data-dependent*: with very unbalanced spectra (e.g., strong C>T@CpG)
#   or very few samples, several rows may correlate similarly and the assignment can flip
#   across resamplings.
#
# If you don't want to use opportunity-normalized profiles change **nmf_input** to mt$nmf_matrix
# below in the code, marked with ##################mt$nmf_matrix################## commments


# --- Convert mm_norm (SomaticSignatures) to SBS-96 names and build nmf_input ---
mm_norm_to_sbs <- function(mm_norm, mt_nmf_matrix) {
  # mm_norm: 96 x S (SomaticSignatures, opportunity-normalized)
  # mt_nmf_matrix: S x 96 (sigminer raw counts)
  X <- as.matrix(mm_norm)
  X <- sweep(X, 2, pmax(colSums(X), 1e-12), "/")  # make columns sum to 1

  Y <- t(as.matrix(mt_nmf_matrix))                   # 96 x S
  Y <- sweep(Y, 2, pmax(colSums(Y), 1e-12), "/")

  # Correlation between 96 rows of X and 96 rows of Y.
  # Some rows can be constant across samples -> sd = 0 -> NA correlations.
  C <- suppressWarnings(cor(t(X), t(Y), use = "pairwise.complete.obs", method = "pearson"))
  C[!is.finite(C)] <- -1  # Replace non-finite with worst similarity (-1), so cost becomes high but finite

  # Build a non-negative cost matrix for Hungarian algorithm.
  # Map correlation in [-1,1] to similarity in [0,1], then cost = 1 - similarity in [0,1].
  sim01 <- (C + 1) / 2
  cost  <- 1 - sim01
  cost[!is.finite(cost)] <- 1

  # Solve assignment (96x96). clue::solve_LSAP expects non-negative costs.
  perm <- clue::solve_LSAP(cost, maximum = FALSE)

  X_sbs <- X
  rownames(X_sbs) <- colnames(mt_nmf_matrix)[perm]
  stopifnot(length(unique(rownames(X_sbs))) == 96)
  X_sbs
}
# sync samples and create NMF input (96 x samples)
common <- intersect(colnames(mm_norm), rownames(mt$nmf_matrix))
mm_norm2 <- mm_norm[, common, drop = FALSE]
mt_mat2  <- mt$nmf_matrix[common, , drop = FALSE]
# Drop samples that are all-zero in either source before matching
keep <- (colSums(as.matrix(mm_norm2), na.rm = TRUE) > 0) & (rowSums(as.matrix(mt_mat2), na.rm = TRUE) > 0)
if (any(!keep)) {
  message("[INFO] Dropping ", sum(!keep), " samples with zero catalog before mapping: ",
          paste(common[!keep], collapse = ", "))
}
mm_norm2 <- mm_norm2[, keep, drop = FALSE]
mt_mat2  <- mt_mat2[keep, , drop = FALSE]

nmf_input <- mm_norm_to_sbs(mm_norm2, mt_mat2)
zero_cols <- colSums(nmf_input, na.rm = TRUE) == 0
if (any(zero_cols)) {
  message("[INFO] Dropping ", sum(zero_cols), " samples with zero catalogue: ",
          paste(colnames(nmf_input)[zero_cols], collapse = ", "))
  nmf_input <- nmf_input[, !zero_cols, drop = FALSE]
}

```

## Signature count (k) selection & COSMIC matching

```{r}
# ============================== BLOCK 5 (FUNCTIONS) ================================
# Signature–COSMIC Similarity Utilities (Cosine & Peak-Sensitive (as in the article))
# ===================================================================================
# Returns the COSMIC SBS reference matrix (96 contexts × COSMIC signatures).
# - Uses GRCh37/hg19 build to match downstream extraction.
# - Output: numeric matrix with rownames as 96 context labels and colnames as COSMIC IDs
.get_cosmic_db <- function() {
  db <- sigminer::get_sig_db("latest_SBS_GRCh37")
  if (is.list(db) && !is.null(db$db)) db <- db$db
  as.matrix(db)}

# --- COSINE -----------------------------------------------------------------------
# Computes standard cosine similarity between extracted signatures and COSMIC.
# Args:
#   sig_obj: result of sigminer::sig_extract (or compatible), containing signatures S.
#   sig_db : COSMIC database key (default GRCh37). 
# Returns: Matrix S × COSMIC with cosine similarities in [0,1].
compute_cosine_matrix <- function(sig_obj, sig_db = "latest_SBS_GRCh37") {
  as.matrix(sigminer::get_sig_similarity(sig_obj, sig_db = sig_db)$similarity)}

# --- PEAK‑SENSITIVE COSINE --------------------------------------------------------
# Compares two signature shapes but looks only at their most telling peaks. 
# Emphasizes informative bins by keeping: bins ≥ thr_ours × max(a) OR ≥ thr_cosmic × max(b)
# Then applies cosine similarity on the masked vectors. Returns NA if vectors are invalid or all-zero under the mask. 
.ps_cosine <- function(a, b, thr_ours = 0.01, thr_cosmic = 0.70) {
  a <- as.numeric(a); b <- as.numeric(b)
  if (!all(is.finite(a)) || !all(is.finite(b))) return(NA_real_)
  if (all(a == 0) || all(b == 0)) return(NA_real_)
  ma <- max(a); mb <- max(b); if (ma == 0 || mb == 0) return(NA_real_)
  mask <- (a >= thr_ours * ma) | (b >= thr_cosmic * mb)
  if (!any(mask)) return(NA_real_)
  aa <- a[mask]; bb <- b[mask]
  den <- sqrt(sum(aa^2) * sum(bb^2)); if (den == 0) return(NA_real_)
  sum(aa * bb) / den
}

# Builds a peak‑sensitive similarity matrix between our signatures and COSMIC.
# Args:
#   prof_mat_96xK: 96 × K matrix of *raw* signature profiles (one column per signature).
#   cosmic_db    : 96 × C COSMIC reference matrix (from .get_cosmic_db()).
#   thr_ours, thr_cosmic: peak thresholds for masking (see .ps_cosine).
# Returns: K × C matrix where [i,j] = PS‑cosine( our Sig i , COSMIC j ).
compute_ps_matrix <- function(prof_mat_96xK, cosmic_db, thr_ours = 0.01, thr_cosmic = 0.70) {
  our <- as.matrix(prof_mat_96xK)
  # Align rows (96 contexts) across our signatures and COSMIC
  common <- intersect(rownames(our), rownames(cosmic_db))
  our <- our[common, , drop = FALSE]
  cos <- cosmic_db[common, , drop = FALSE]
  # Normalize our profiles per column to sum 1 (frequency-like)
  our <- sweep(our, 2, pmax(colSums(our, na.rm = TRUE), 1e-12), "/")
  # Compute PS‑cosine for every pair: our signatures × COSMIC signatures
  ps <- outer(seq_len(ncol(our)), seq_len(ncol(cos)), Vectorize(function(i, j) {
    .ps_cosine(our[, i], cos[, j], thr_ours = thr_ours, thr_cosmic = thr_cosmic)
  }))
  dimnames(ps) <- list(colnames(our), colnames(cos))
  as.matrix(ps)
}

# --- best_top_k -----------------------------------------------------------------
# Returns top‑k COSMIC matches per our signature from a similarity matrix.
# Args:
#   sim_mat: S × C matrix of similarities (rows = our signatures, cols = COSMIC IDs).
#   k      : how many best matches to return for each signature.
# Output: Long data.frame with columns: Signature, Rank, COSMIC, Similarity.
best_top_k <- function(sim_mat, k = 3) {
  do.call(rbind, lapply(seq_len(nrow(sim_mat)), function(i) {
    r <- sim_mat[i, ]
    ord <- order(r, decreasing = TRUE, na.last = NA)
    kk <- min(k, length(ord))
    data.frame(
      Signature = rep(rownames(sim_mat)[i], kk),
      Rank      = seq_len(kk),
      COSMIC    = colnames(sim_mat)[ord][seq_len(kk)],
      Similarity= as.numeric(r[ord][seq_len(kk)]),
      stringsAsFactors = FALSE)
  }))
}

# --- wide table: COSMIC × (Cosine_S1, PeakSensitive_S1, Cosine_S2, ...) --------
# Produces a wide comparison table: one row per COSMIC signature, paired columns per our signature.
build_similarity_table_wide <- function(cos_mat, ps_mat) {
  common <- intersect(colnames(cos_mat), colnames(ps_mat))
  our_sigs <- rownames(cos_mat)
  out <- data.frame(COSMIC = common, stringsAsFactors = FALSE)
  for (s in our_sigs) {
    out[[paste0("Cosine_", s)]]        <- as.numeric(cos_mat[s, common])
    out[[paste0("PeakSensitive_", s)]] <- as.numeric(ps_mat[s,  common])}
  out
}
```

```{r}
# ============================== BLOCK 6 ==============================
# Selecting Number of Signatures (k)
#
# Evaluating k ∈ {2..7} by NMF rank survey (cophenetic, RSS, etc.) 
# and matching to COSMIC (cosine and peak‑sensitive)
# ======================================================================

# Extract one NMF solution for a given k and return a compact bundle
extract_k <- function(nmf_matrix, k, n_runs = 30L, seed = 42) {
  nm <- as.matrix(nmf_matrix)
  storage.mode(nm) <- "double"
  nm[!is.finite(nm)] <- 0
  set.seed(seed)
  sig_k <- sigminer::sig_extract(nm, n_sig = as.integer(k), nrun = as.integer(n_runs), cores = 1, use_conda = FALSE)
  list(
    sig  = sig_k,                                                        # sigminer object
    expo = as.matrix(sigminer::get_sig_exposure(sig_k)),                 # S × N (exposures)
    prof = as.matrix(sigminer::sig_signature(sig_k, normalize = "raw"))) # 96 × S (profiles)
}

# Save per‑k outputs under k_* directory/: profiles, similarity tables, and top matches
save_k_bundle <- function(k, bundle, cos_mat, ps_mat, out_dir) {
  kdir <- file.path(out_dir, sprintf("k_%d", k)); dir.create(kdir, showWarnings = FALSE, recursive = TRUE)
  # 1) Mutational signatures profiles (96 × S) 
  write.csv(bundle$prof, file.path(kdir, "signature_profiles_96bins.csv"))
  # 2) Cosine + Peak‑Sensitive similarity to COSMIC (wide table)
  wide_tbl <- build_similarity_table_wide(cos_mat, ps_mat)
  write.csv(wide_tbl, file.path(kdir, "cosmic_similarity_wide.csv"), row.names = FALSE)
  # 3) Top‑3 COSMIC hits by each metric
  write.csv(best_top_k(cos_mat, 3), file.path(kdir, "top3_cosine.csv"), row.names = FALSE)
  write.csv(best_top_k(ps_mat,  3), file.path(kdir, "top3_peak_sensitive.csv"), row.names = FALSE)
  # 4) Exposures
  write.csv(bundle$expo, file.path(kdir, "exposures.csv"))
}


# --- SETUP ---------------------------------------------------------
k_range     <- 2:7
n_runs_sweep <- 500L # Fixed number of NMF restarts for the sweep 
cosmic_db   <- .get_cosmic_db()

# --- Cophenetic correlation via NMF::nmfEstimateRank --------------
# Fit NMF across k_range (96×samples input) and export the diagnostic multi‑panel plot.
# NMF expects a matrix with features (96 contexts) in rows and samples in columns.

##################mt$nmf_matrix##################
# If you want to use raw counts without opportunity-normalizstion
# uncomment next 3 lines and after comment next 6, change k_use to k_range in nmf_est

# # Transpose mt$nmf_matrix (samples×96) → V (96×samples) and sanitize values.
# V <- t(as.matrix(mt$nmf_matrix))
# V[!is.finite(V) | V < 0] <- 0

# After (uses normalized, SBS-aligned profiles from nmf_input)
V <- as.matrix(nmf_input)  # 96 x samples
V[!is.finite(V) | V < 0] <- 0
V <- V[rowSums(V) > 0, colSums(V) > 0, drop = FALSE]
V[V == 0] <- .Machine$double.eps
k_use <- k_range[k_range < min(nrow(V), ncol(V))] # trim k to valid range


set.seed(42)
nmf_est <- NMF::nmfEstimateRank(V, range = k_use, method = "brunet", nrun = n_runs_sweep,
                                seed = 42, .options = "t")

# --- Multi-panel rank survey from NMF ----
p_rank <- NMF::plot(nmf_est, what = c("cophenetic","dispersion","evar","residuals","rss","silhouette","sparseness"))
pdf(file.path(out_dir, "k_sweep_NMF_estimateRank_overview.pdf"), width = 9, height = 7)
print(p_rank)
dev.off()


# Helper: from an S×COSMIC matrix, compute the mean of each signature’s best COSMIC match (higher is better).
.mean_best <- function(sim_mat) {
  if (is.null(sim_mat) || length(sim_mat) == 0) return(NA_real_)
  row_max <- apply(sim_mat, 1, function(r) suppressWarnings(max(as.numeric(r), na.rm = TRUE)))
  mean(row_max[is.finite(row_max)], na.rm = TRUE)}

# Main loop: for each k, fit NMF, compare to COSMIC (cosine + PS), save outputs, and record metrics
k_results <- lapply(k_range, function(k) {
  message("  [k-sweep] extracting k=", k) 
  ##################mt$nmf_matrix################## 
  # If you want to use raw counts without opportunity-normalizstion 
  # uncomment next line and after comment next 1 
  # bundle <- extract_k(mt$nmf_matrix, k, n_runs = n_runs_sweep) 
  bundle <- extract_k(t(nmf_input), k, n_runs = n_runs_sweep) 
  cos_mat <- compute_cosine_matrix(bundle$sig)
  ps_mat  <- compute_ps_matrix(bundle$prof, cosmic_db, thr_ours = 0.01, thr_cosmic = 0.70)
  save_k_bundle(k, bundle, cos_mat, ps_mat, out_dir)
  data.frame(k = k, mean_best_cos = .mean_best(cos_mat), mean_best_ps  = .mean_best(ps_mat))
})

# Summarize metrics across k
k_df <- do.call(rbind, k_results)
k_df_out <- data.frame(
  k = k_df$k,
  mean_best_cosine = k_df$mean_best_cos,
  mean_best_peak_sensitive = k_df$mean_best_ps)
utils::write.csv(k_df_out, file.path(out_dir, "k_sweep_mean_best_cosines.csv"), row.names = FALSE)

# Plot metrics vs k and mark the suggested k per metric
best_k_cos <- k_df$k[which.max(k_df$mean_best_cos)]
best_k_ps  <- k_df$k[which.max(k_df$mean_best_ps)]
pdf(file.path(out_dir, "k_sweep_mean_best_cosines.pdf"), width = 7.5, height = 4.2)
print(
  ggplot(k_df, aes(x = k)) +
    geom_line(aes(y = mean_best_cos, colour = "Cosine")) +
    geom_point(aes(y = mean_best_cos, colour = "Cosine")) +
    geom_line(aes(y = mean_best_ps, colour = "Peak_sensitive_cosine"), linetype = "dashed") +
    geom_point(aes(y = mean_best_ps, colour = "Peak_sensitive_cosine"), shape = 1) +
    geom_vline(xintercept = best_k_cos, linetype = "dotted") +
    geom_vline(xintercept = best_k_ps,  linetype = "dashed") +
    scale_colour_discrete(name = "Metric") +
    labs(x = "n_sig (k)", y = "Mean of best similarity",
         title = "Mean(best similarity) vs number of signatures") +
    theme_bw())
dev.off()
```

## Final k = 3 signature extraction

```{r}
# ============================== BLOCK 7 ==============================
# FINAL SIGNATURE EXTRACTION FOR A SINGLE k
# ====================================================================

# ----- Final run parameters -----
n_sig_final  <- 3L        # number of signatures in the final model
nrun_final   <- 1000L     # number of NMF restarts for robustness
out_dir_final <- file.path(out_dir, "finalK") # separate folder to avoid overwriting sweep results
if (!dir.exists(out_dir_final)) dir.create(out_dir_final, recursive = TRUE, showWarnings = FALSE)

# Extract the final model (NMF) at the chosen k
message("[FINAL K] Extracting signatures: k = ", n_sig_final, ", nrun=", nrun_final)
##################mt$nmf_matrix################## 
# If you want to use raw counts without opportunity-normalizstion 
# uncomment next line and after comment next 1 
# final_bundle <- extract_k(mt$nmf_matrix, k = n_sig_final, n_runs = nrun_final, seed = 42)
final_bundle <- extract_k(t(nmf_input), k = n_sig_final, n_runs = nrun_final, seed = 42)

# Compare the extracted signatures to COSMIC 
cosmic_db <- .get_cosmic_db()
cos_mat   <- compute_cosine_matrix(final_bundle$sig, sig_db = "latest_SBS_GRCh37")
ps_mat    <- compute_ps_matrix(final_bundle$prof, cosmic_db, thr_ours = 0.01, thr_cosmic = 0.70)

# Persist all artifacts for downstream analyses
save_k_bundle(n_sig_final, final_bundle, cos_mat, ps_mat, out_dir = out_dir_final)

# Visualize profiles (COSMIC‑style facets) 
sig <- final_bundle$sig
pdf(file.path(out_dir_final, sprintf("k_%d", n_sig_final), "signature_profiles.pdf"), width = 11, height = 7)
sigminer::show_sig_profile(sig, mode = "SBS", style = "cosmic")
dev.off()

# Visualize sample‑level exposures in COSMIC‑style layout
pdf(file.path(out_dir_final, sprintf("k_%d", n_sig_final), "signature_exposures_cosmic.pdf"), width = 12, height = 6)
sigminer::show_sig_exposure(sig, style = "cosmic")
dev.off()

message("[FINAL K] Done. Results saved to ", file.path(out_dir_final, sprintf("k_%d", n_sig_final)))
```

## Clustering & Figure 3A–style heatmap

```{r}
# ============================== BLOCK 8 =====================================
# Figure 3A–style heatmap
# ============================================================================
# Heatmap of normalized trinucleotide mutational profiles
# with hierarchical clustering, automatic k selection (DynamicTreeCut vs
# silhouette), and robustness/stability diagnostics (ARI across metrics)
#
# Workflow
#   1) Row‑wise Z‑scaling of the 96 contexts → matrix mm_z.
#   2) Helper utilities: column‑wise distance, cluster palette.
#   3) auto_clusters(): choose cluster labels by comparing DynamicTreeCut vs
#      the best silhouette over k = 2..k_max (Pearson + complete linkage).
#   4) Build a ComplexHeatmap with a single column dendrogram, 
#      color dendrogram branches by the final cluster labels, and
#      show a top annotation bar with cluster colors.
#   5) Robustness block: repeat clustering across distance/linkage families, pick
#      the auto solution for each pair, and summarize (CSV) + visualize an ARI
#      (Adjusted Rand Index) matrix showing agreement between solutions
# ============================================================================

## Row‑wise Z‑scaling (each of the 96 contexts is centered & scaled)
mm_z <- t(scale(t(as.matrix(mm_norm))))
mm_z[!is.finite(mm_z)] <- 0  # replace NaN/Inf

## Row labels: annotate only the top 20 % contexts by mean Z-score
row_means  <- rowMeans(abs(mm_z), na.rm = TRUE)
threshold  <- quantile(row_means, 0.8, na.rm = TRUE)
row_labels <- ifelse(row_means > threshold, rownames(mm_z), "")

## Colour ramp for Z-scores (blue – cyan – orange)
rng <- range(mm_z, na.rm = TRUE)
col_fun <- circlize::colorRamp2(c(rng[1], mean(rng), rng[2]),
  c("#1f3b87", "#3fb6d0", "#ff6a00"))


## --- HELPERS ----------------------------------------------------------------
## .dist_cols(): distance between columns (samples) under several metrics.
##       - Pearson/Spearman use pairwise complete observations
##       - Cosine distance is 1 − cosine similarity (numerically clamped to [0,2])
.dist_cols <- function(M, method = c("euclidean","pearson","spearman","cosine")){
  method <- match.arg(method)
  if (method == "euclidean") return(dist(t(M), method = "euclidean"))
  if (method == "pearson")  return(as.dist(1 - cor(M, method = "pearson",  use = "pairwise.complete.obs")))
  if (method == "spearman") return(as.dist(1 - cor(M, method = "spearman", use = "pairwise.complete.obs")))
  if (method == "cosine"){
    X <- as.matrix(M); X[!is.finite(X)] <- 0
    nrm <- sqrt(colSums(X^2)); nrm[nrm == 0] <- 1
    Xn <- sweep(X, 2, nrm, "/")
    S  <- crossprod(Xn)
    return(as.dist(1 - pmin(pmax(S, -1), 1)))
  }
}

## Generate an arbitrary length palette (Set2 + interpolation)
.cluster_palette <- function(k){
  base <- RColorBrewer::brewer.pal(8, "Set2")
  if (k <= 8) base[seq_len(k)] else colorRampPalette(base)(k)}


## --- CLUSTER SELECTION ------------------------------------------------------
## Automatic cluster selection (Pearson + complete linkage)
##    Chooses between: (A) DynamicTreeCut and (B) best silhouette over k=2..k_max.
##    Tie‑break rule: prefer DTC if its mean silhouette is within sil_tol of the
##    silhouette optimum. Returns: hc (tree), cl (labels), k, and diagnostics.
auto_clusters <- function(M, k_max = 8, deepSplit = 3, minClusterSize = 15, sil_tol = 0.02){
  D  <- .dist_cols(M, "pearson")
  hc <- hclust(D, method = "complete")

  # (A) Dynamic tree cut — automatically finds branches subject to minClusterSize
  cl_dtc <- dynamicTreeCut::cutreeDynamic(
    dendro = hc, distM = as.matrix(D),
    deepSplit = deepSplit, pamRespectsDendro = TRUE,
    minClusterSize = minClusterSize)
  k_dtc   <- length(unique(cl_dtc))
  sil_dtc <- tryCatch(mean(silhouette(cl_dtc, D)[,3]), error = function(e) NA_real_)

  # (B) Best silhouette across k = 2..k_max
  ks   <- 2:min(k_max, ncol(M)-1)
  sils <- sapply(ks, function(k){
    cl <- cutree(hc, k = k)
    mean(silhouette(cl, D)[,3])
  })
  k_sil    <- ks[which.max(sils)]
  sil_best <- max(sils)
  cl_sil   <- cutree(hc, k = k_sil)

  # Decision: pick DTC if it is “close enough” to the silhouette optimum
  use_dtc  <- is.finite(sil_dtc) && k_dtc >= 2 && (sil_dtc >= (sil_best - sil_tol))
  cl_final <- if (use_dtc) cl_dtc else cl_sil
  k_final  <- length(unique(cl_final))

  list(hc=hc, cl=cl_final, k=k_final, use_dtc=use_dtc, k_dtc=k_dtc, sil_dtc=sil_dtc,
       k_sil=k_sil, sil_best=sil_best)
}


## --- CLUSTERING --------------------------------------------------------------
## Perform clustering on Z‑scaled data
cl_res <- auto_clusters(mm_z)

cat(sprintf(
  "Selected %d clusters  (DynamicTreeCut: k=%d, mean silhouette=%.3f; ",
  cl_res$k, cl_res$k_dtc, cl_res$sil_dtc))
cat(sprintf(
  "Best silhouette: k=%d, mean=%.3f; chosen=%s)\n",
  cl_res$k_sil, cl_res$sil_best,
  ifelse(cl_res$use_dtc, "DynamicTreeCut", "Silhouette")))

## Factor of cluster labels and palette for annotations/branches
grp <- factor(cl_res$cl, levels = sort(unique(cl_res$cl)))
names(grp) <- colnames(mm_z)
pal <- .cluster_palette(cl_res$k)

## Color the column dendrogram by final cluster labels
dend <- as.dendrogram(cl_res$hc)
cl_ids <- setNames(as.integer(grp), names(grp))
clusters_vec <- cl_ids[labels(dend)]
dend <- dendextend::color_branches(dend, clusters = clusters_vec, col = pal)

## Top annotation: one color bar indicating cluster membership per sample
ta <- HeatmapAnnotation(
  Cluster = grp,
  col = list(Cluster = setNames(pal, levels(grp))),
  simple_anno_size = unit(4, "mm"),
  annotation_name_side = "left",
  annotation_name_gp = gpar(fontsize = 9))

## Build the heatmap 
ht <- Heatmap(
  mm_z,
  name = "Z-score",
  col = col_fun,
  cluster_rows = FALSE,
  cluster_columns = dend,
  column_dend_height = unit(30, "mm"),
  column_dend_gp = gpar(lwd = 1.2),
  top_annotation = ta,
  show_row_names = TRUE,
  row_labels = row_labels,
  row_names_gp = gpar(fontsize = 3.5),
  show_column_names = FALSE,
  heatmap_width  = unit(240, "mm"),
  heatmap_height = unit(120, "mm"))

## Draw and save the heatmap
draw(ht, heatmap_legend_side = "right", annotation_legend_side = "right")
pdf(file.path(out_dir, "mutational_profiles_heatmap.pdf"), width = 12, height = 7)
draw(ht, heatmap_legend_side = "right", annotation_legend_side = "right")
dev.off()

## Write cluster assignments (sample → cluster)
cl_tbl <- data.frame(sample = colnames(mm_z), cluster = grp)
write.csv(cl_tbl, file.path(out_dir, "cluster_assignments.csv"), row.names = FALSE)

```

## Clustering robustness block

```{r}
# ============================== BLOCK 9 =====================================
# ROBUSTNESS BLOCK 
# ============================================================================
# For each metric/linkage pair, we auto‑select clusters (DTC vs silhouette),
# summarize quality, and visualize agreement (ARI heatmap).
# ============================================================================

# Universal auto‑clustering for any metric/linkage pair
.auto_clusters_ml <- function(M, metric, linkage, k_max = 8, deepSplit = 2, minClusterSize = 10, sil_tol = 0.02){
  D  <- .dist_cols(M, metric)
  if (linkage == "ward.D2" && attr(D, "method") != "euclidean") return(NULL) # ward.D2 ↔ Euclidean only
  hc <- hclust(D, method = linkage)
  
  # DynamicTreeCut solution for this pair
  cl_dtc <- dynamicTreeCut::cutreeDynamic(
    dendro = hc, distM = as.matrix(D),
    deepSplit = deepSplit, pamRespectsDendro = TRUE, minClusterSize = minClusterSize)
  k_dtc   <- length(unique(cl_dtc))
  sil_dtc <- suppressWarnings(mean(silhouette(cl_dtc, D)[,3]))
  
  # Best silhouette across k for this pair
  ks <- 2:min(k_max, ncol(M)-1); if (!length(ks)) return(NULL)
  sils <- sapply(ks, function(k){ cl <- cutree(hc, k = k); suppressWarnings(mean(silhouette(cl, D)[,3])) })
  k_sil <- ks[which.max(sils)]; sil_best <- max(sils); cl_sil <- cutree(hc, k = k_sil)
  
  # Choose between DTC and silhouette (within sil_tol of the best)
  use_dtc <- is.finite(sil_dtc) && k_dtc >= 2 && (sil_dtc >= (sil_best - sil_tol))
  cl_final <- if (use_dtc) cl_dtc else cl_sil
  
  list(metric=metric, linkage=linkage, cl_final=cl_final,
       k_final=length(unique(cl_final)),
       k_dtc=k_dtc, sil_dtc=sil_dtc, k_sil=k_sil, sil_best=sil_best,
       sil_auto=if (use_dtc) sil_dtc else sil_best, use_dtc=use_dtc)
}

# Grid of metric/linkage combinations we want to probe
metrics  <- c("pearson","spearman","cosine","euclidean")
linkages <- c("complete","average","ward.D2")
k_max_r  <- 8; deepSplit_r <- 2; minClSize_r <- 10; sil_tol_r <- 0.02

runs <- list()
for (met in metrics){
  for (lnk in linkages){
    if (lnk == "ward.D2" && met != "euclidean") next
    rr <- .auto_clusters_ml(mm_z, metric = met, linkage = lnk,
                            k_max = k_max_r, deepSplit = deepSplit_r,
                            minClusterSize = minClSize_r, sil_tol = sil_tol_r)
    if (!is.null(rr)) runs[[paste(met, lnk, sep=":")]] <- rr
  }
}

# ---- robustness_summary.csv (one row per metric/linkage pair)
robust_df <- do.call(rbind, lapply(runs, function(r)
  data.frame(metric=r$metric, linkage=r$linkage,
             k_auto=r$k_final,
             chooser=ifelse(r$use_dtc,"DynamicTreeCut","Silhouette"),
             mean_silhouette=r$sil_auto,
             k_dtc=r$k_dtc, sil_dtc=r$sil_dtc,
             k_sil=r$k_sil, sil_best=r$sil_best,
             stringsAsFactors = FALSE)))
robust_df <- robust_df[order(-robust_df$mean_silhouette, robust_df$metric, robust_df$linkage),]
write.csv(robust_df, file.path(out_dir, "robustness_summary.csv"), row.names = FALSE)

# ---- robustness_ARI_heatmap.pdf (agreement between auto solutions)
keys <- names(runs)
if (length(keys) >= 2){
  ari_m <- matrix(NA_real_, nrow = length(keys), ncol = length(keys),
                  dimnames = list(keys, keys))
  for (i in seq_along(keys)) for (j in seq_along(keys))
    ari_m[i,j] <- {
      a <- as.integer(as.factor(runs[[keys[i]]]$cl_final))
      b <- as.integer(as.factor(runs[[keys[j]]]$cl_final))
      # ARI
      tab <- table(a,b); n <- sum(tab)
      comb2 <- function(x) sum(choose(x,2))
      c_ind <- comb2(tab); a_sum <- comb2(rowSums(tab)); b_sum <- comb2(colSums(tab))
      denom <- 0.5*(a_sum + b_sum) - (a_sum*b_sum)/choose(n,2)
      if (denom == 0) NA_real_ else (c_ind - (a_sum*b_sum)/choose(n,2))/denom
    }

  ht_ari <- Heatmap(
    ari_m,
    name = "ARI",
    col = colorRamp2(seq(0, 1, length.out = 7), RColorBrewer::brewer.pal(7, "Greens")),
    cluster_rows = FALSE, cluster_columns = FALSE,
    column_title = "Adjusted Rand Index between metric/linkage combinations",
    row_names_gp = gpar(fontsize = 7), column_names_gp = gpar(fontsize = 7),
    width = unit(160, "mm"), height = unit(160, "mm"))
  pdf(file.path(out_dir, "robustness_ARI_heatmap.pdf"), width = 8.5, height = 8.5)
  draw(ht_ari, heatmap_legend_side = "right")
  dev.off()
}

```

## Dominant Signature per Cluster

```{r}
# ============================== BLOCK 10 =====================================
# Dominant Signature per Cluster
# =============================================================================

## Read and clean data
in_expo <- "/Users/elizaavveettaaa/Desktop/Internship_outputs/finalK/k_3/exposures.csv"
in_clst <- "/Users/elizaavveettaaa/Desktop/Internship_outputs/cluster_assignments.csv"

expo <- read.csv(in_expo, check.names = FALSE, stringsAsFactors = FALSE)
expo   <- expo[, -1]          # Drop the emplty first column 
rownames(expo) <- expo$sample # Use 'sample' as row identifier
expo   <- expo[, names(expo) != "sample", drop = FALSE]  # drop 'sample'column from the matrix
expo[] <- lapply(expo, as.numeric) # Coerce exposures to numeric

clu <- read.csv(in_clst, stringsAsFactors = FALSE)
rownames(clu) <- clu$sample
clu <- clu[, "cluster", drop = FALSE]

# Keep only samples present in both tables
common <- intersect(rownames(expo), rownames(clu))
expo   <- expo[common, , drop = FALSE]
clu    <- clu [common, , drop = FALSE]

## Normalize exposures and find the dominant signature per sample 
row_sums <- rowSums(expo); row_sums[row_sums == 0] <- 1 # avoid division by zero
P  <- expo / row_sums                                   # fraction of each signature per sample

imax <- max.col(as.matrix(P), ties.method = "first")      # index of max element per row
dominant_signature <- colnames(P)[imax]
dominant_fraction  <- P[cbind(seq_len(nrow(P)), imax)]

# Combine sample-level results into one data frame and sort by cluster & dominance strength
sample_level <- data.frame(
  sample = rownames(P), 
  cluster = clu$cluster, 
  dominant_signature, 
  dominant_fraction,
  P,
  stringsAsFactors = FALSE)
write.csv(sample_level,file.path(out_dir, "samples_with_dominant_signature.csv"), row.names = FALSE)

## --- Summarize by cluster ------------------------------------------------
cluster_ids <- sort(unique(clu$cluster))

# Mean fraction of each signature in each cluster
mean_by_cluster <- data.frame(
  cluster = cluster_ids,
  t(vapply(cluster_ids, function(k) colMeans(P[clu$cluster == k, , drop = FALSE]), numeric(ncol(P)))))
names(mean_by_cluster)[-1] <- colnames(P)

# Identify the signature with highest mean for each cluster
mean_by_cluster$dominant_by_mean <- colnames(P)[apply(mean_by_cluster[, -1], 1, which.max)]
write.csv(mean_by_cluster, file.path(out_dir, "cluster_mean_exposures.csv"), row.names = FALSE)

# Majority vote: most frequently dominant signature in each cluster
tab <- table(sample_level$cluster, sample_level$dominant_signature)
dominant_counts <- as.data.frame.matrix(tab)
dominant_counts$cluster <- as.integer(rownames(dominant_counts))
dominant_counts$majority_signature <- colnames(tab)[apply(tab, 1, which.max)]
write.csv(dominant_counts,file.path(out_dir, "cluster_majority_dominant_counts.csv"),row.names = FALSE)

## --- Visualizations ------------------------------------------------
# Heatmap of cluster mean exposures
hm <- as.matrix(mean_by_cluster[, colnames(P), drop = FALSE])
rownames(hm) <- paste0("Cluster_", mean_by_cluster$cluster)
pdf(file.path(out_dir, "heatmap_cluster_mean_exposures.pdf"))
pheatmap(hm, cluster_rows = FALSE, cluster_cols = FALSE, main = "Mean signature fractions by cluster")
dev.off()

# Stacked bar chart of sample exposures, ordered within clusters
p_df <- cbind(P, sample = rownames(P), cluster = factor(clu$cluster))
ord <- match(sample_level$sample, rownames(p_df))
p_df <- p_df[ord, , drop = FALSE]
p_df$sample <- factor(p_df$sample, levels = p_df$sample)
long <- melt(p_df, id.vars = c("sample","cluster"), variable.name = "Signature", value.name = "Fraction")

pdf(file.path(out_dir, "stacked_by_sample_sorted_within_cluster.pdf"), width = 12, height = 7)
ggplot(long, aes(sample, Fraction, fill = Signature)) +
  geom_bar(stat = "identity", width = 0.9) +
  facet_grid(~ cluster, scales = "free_x", space = "free_x") +
  labs(title = "Signature exposures per sample by cluster",
       x = "Sample", y = "Fraction") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_rect(fill = "grey90", colour = NA))
dev.off()

## --- Quick textual recap ------------------------------------------------
cat("\n=== Cluster summary ===\n")
for (k in cluster_ids) {
  avg <- mean_by_cluster[mean_by_cluster$cluster == k, colnames(P), drop = TRUE]
  maj <- dominant_counts$majority_signature[dominant_counts$cluster == k]
  cat(sprintf("Cluster %s: dominant-by-mean = %s; majority vote = %s; n = %d\n",
              k, names(which.max(avg)), maj, sum(clu$cluster == k)))
}
```

## KM survival curves by clusters

```{r}
# ============================== BLOCK 11 =====================================
# Kaplan–Meier survival curves by clusters
# =============================================================================

# --- LOAD CLINICAL DATA ---------------------------------------
excel_sheet3 <- 3
message("[INFO] Reading Excel sheet ", excel_sheet3, ": ", excel_path)
clin_xl <- read_excel(path.expand(excel_path), sheet = excel_sheet3, skip = 2, col_names = TRUE)  
# turn string "NA" into proper NA across character columns
clin_xl <- clin_xl %>% mutate(across(where(is.character), ~ na_if(., "NA")))

# Cluster assignments
cluster_df <- data.frame(sample  = rownames(clu), cluster = clu$cluster, stringsAsFactors = FALSE)

# Merge clinical data with clusters (inner join: keep only samples present in both tables)
km_df <- merge(clin_xl[, c("Tumor_Sample_Barcode", "SURVIVAL_TIME_60months", "patient.vital_status_60months")], cluster_df, by.x = "Tumor_Sample_Barcode", by.y = "sample")

# Ensure proper types 
km_df$SURVIVAL_TIME_60months <- as.numeric(as.character(km_df$SURVIVAL_TIME_60months))
km_df$patient.vital_status_60months <- as.integer(as.character(km_df$patient.vital_status_60months))

# Keep complete cases and label clusters (pretty labels for legend)
km_df <- km_df[complete.cases(km_df[, c("SURVIVAL_TIME_60months",
                                        "patient.vital_status_60months",
                                        "cluster")]), ]
km_df$cluster <- factor(km_df$cluster,
                        levels = sort(unique(km_df$cluster)),
                        labels = paste("Cluster", sort(unique(km_df$cluster))))

# Fit KM curves and compute p-values
fit <- survfit(Surv(SURVIVAL_TIME_60months, patient.vital_status_60months) ~ cluster, data = km_df)

# Robust pairwise log-rank p-values (compute per pair on a subset; avoids name/indexing issues)
lev <- levels(km_df$cluster)
if (length(lev) >= 2) {
  pairs <- utils::combn(lev, 2, simplify = FALSE)
  pair_labels <- vapply(pairs, function(ab) {
    sub_df <- km_df[km_df$cluster %in% ab, ]
    s2 <- survival::survdiff(
      survival::Surv(SURVIVAL_TIME_60months, patient.vital_status_60months) ~ cluster,
      data = sub_df)
    p <- 1 - pchisq(s2$chisq, length(s2$n) - 1)
    sprintf("p-value (%s vs %s) = %s",
            sub("Cluster ", "", ab[1]),
            sub("Cluster ", "", ab[2]),
            format.pval(p, digits = 3))
  }, FUN.VALUE = character(1))
} else {pair_labels <- character(0)}

# Plot KM curves with risk table. Global p-value and method are auto-rendered by survminer
plt <- ggsurvplot(
  fit, data = km_df,
  risk.table = TRUE,
  pval = TRUE, 
  pval.method = TRUE,
  conf.int = FALSE,
  censor = TRUE,
  legend.title = "",
  legend.labs = lev,
  xlab = "Time (months)",
  ylab = "Survival probability",
  ggtheme = theme_minimal(base_size = 12))

# Add pairwise p-values in the bottom-right corner as manual text annotations.
# NOTE: Manual text annotation may trigger harmless ggplot warnings like
# "Removed X rows containing missing values or values outside the scale range (geom_text())"
# when labels fall partially outside the panel
xpos <- max(km_df$SURVIVAL_TIME_60months, na.rm = TRUE) * 0.60
for (k in seq_along(pair_labels)) {
  plt$plot <- plt$plot + annotate("text", x = xpos, y = 0.35 - (k - 1) * 0.07, 
                                  label = pair_labels[k], hjust = 0, size = 3)
}

# Save as PDF
pdf(file.path(out_dir, "kaplan_meier_by_cluster.pdf"), width = 7.5, height = 6.5)
suppressWarnings(print(plt))  # suppress warnings caused by manual text annotations 
dev.off()
```

## Clinicogenomic heatmap 

```{r}
# ============================== BLOCK 12 =====================================
# Composite heatmap of Clinicogenomic heatmap
# =============================================================================
# Composite heatmap of sample annotations (top) and z-scored numeric tracks (bottom)

# Columns to show as categorical annotations (subset of clin_xl columns)
anno_cols <- intersect(names(clin_xl), c("Subtype_Morphology", "gender", "msi_mss", "Group_Wnt"))
# Keep original sample order; drop rows with missing gender; convert NAs in
# selected annotation columns to explicit "Unknown" labels for consistent plotting
clin <- clin_xl %>%
  distinct(Tumor_Sample_Barcode, .keep_all = TRUE) %>%
  dplyr::filter(!is.na(gender)) %>%
  mutate(across(all_of(anno_cols), ~ replace_na(as.character(.), "Unknown")))

# Base "placeholder" matrix: 1 row × N samples, with X-axis = Tumor_Sample_Barcode.
# This serves only to anchor the top annotation bar against the sample order.
sample_ids <- clin$Tumor_Sample_Barcode
base_mat <- matrix(0, nrow = 1, ncol = length(sample_ids), dimnames = list(" ", sample_ids))

# ==================== COLOR ENGINE ====================
# Given a vector of values and a palette (vector of colors), return a named color map.
# Ensures the level "Unknown" appears last in legends by reordering levels.
make_color_map <- function(values, pal) {
  lvls <- unique(values)
  if ("Unknown" %in% lvls) lvls <- c(lvls[lvls != "Unknown"], "Unknown")
  cols <- pal            # expected: length(pal) == length(lvls)
  names(cols) <- lvls
  cols
}
# Data frame of annotations in the preserved sample order
anno_df <- clin[, anno_cols, drop = FALSE] 

# --- PALETTES (manually, one vector per annotation column) ---
# Provide exactly as many colors as there are unique levels in the corresponding
# column of `anno_df` (after "Unknown" imputation).
palettes <- list(
  Subtype_Morphology = c("#F09B77", "khaki", "#5FB8A5", "azure2"),
  gender             = c("#F48FB1", "#81D4FA"),
  msi_mss            = c("#E64B35", "azure2"),
  Group_Wnt          = c("darkblue", "azure2"))

# Build list of named color vectors for each annotation column using the provided palettes
col_list <- lapply(names(anno_df), function(nm) make_color_map(anno_df[[nm]], palettes[[nm]]))
names(col_list) <- names(anno_df)

# Lock factor levels to the color mapping order so legends and tiles align exactly
for (nm in names(anno_df)) {
  anno_df[[nm]] <- factor(anno_df[[nm]], levels = names(col_list[[nm]]))
}

# ==================== HEATMAP ====================
# Top annotation bar built from the categorical annotation data frame
# - `annotation_legend_param` gives each legend a readable title equal to the column name
# - `simple_anno_size` sets compact tile height for the annotation rows

top_ha <- HeatmapAnnotation(
  df  = anno_df,
  col = col_list,
  simple_anno_size = unit(4, "mm"),
  annotation_name_side = "left",
  annotation_legend_param = structure(lapply(names(anno_df), function(nm) list(title = nm)),
                                      names = names(anno_df)))
# Minimal 1×N heatmap acting as a canvas for the top annotations
ht_2 <- Heatmap(
  base_mat, name = NULL,
  col = c("0" = "white"),
  top_annotation = top_ha,
  cluster_columns = FALSE,
  cluster_rows = FALSE,
  show_row_names = FALSE,
  show_column_names = FALSE,                
  show_heatmap_legend = FALSE)

# ---- LOWER BLOCK: Mutations / Deletions / Amplifications (numeric tracks) ----
# Extract and coerce numeric features. Keep only samples present in the base matrix
# and then hard-align the column order to `sample_ids`.
num_tbl <- clin_xl %>%
  dplyr::transmute(
    Tumor_Sample_Barcode,
    Mutations      = as.numeric(mutation_per_MB_nonsilent),
    Deletions      = as.numeric(Del),
    Amplifications = as.numeric(Amp)
  ) %>%
  dplyr::filter(Tumor_Sample_Barcode %in% sample_ids)

# Ensure exact column order matches the main heatmap's samples
num_tbl <- num_tbl[match(sample_ids, num_tbl$Tumor_Sample_Barcode), ]

# Build a 3×N matrix (rows: metrics; columns: samples), then z-score per row
bot_mat_raw <- t(as.matrix(num_tbl[, c("Mutations","Deletions","Amplifications")]))
colnames(bot_mat_raw) <- sample_ids
rownames(bot_mat_raw) <- c("Mutations","Deletions","Amplifications")

# Row-wise z-score with NA-robustness. If sd==0 or not finite, return zeros to avoid NaN/Inf.
z_row <- function(x) {
  m <- mean(x, na.rm = TRUE); s <- sd(x, na.rm = TRUE)
  if (!is.finite(s) || s == 0) return(rep(0, length(x)))
  (x - m) / s
}
bot_mat <- t(apply(bot_mat_raw, 1, z_row))

# --- ASYMMETRIC AUTO-CLIP by quantiles: 5% for negatives, 95% for positives ---
# This caps extreme values while preserving potential asymmetry between left and right tails.
# Returns the clipped matrix, a white→red color ramp, and the numeric limits used.
auto_diverging <- function(zmat, q_neg = 0.05, q_pos = 0.95) {
  neg <- zmat[zmat < 0]; pos <- zmat[zmat > 0]
  lo <- if (length(neg)) unname(quantile(neg, q_neg, na.rm = TRUE)) else 0
  hi <- if (length(pos)) unname(quantile(pos, q_pos, na.rm = TRUE)) else 1
  zmat <- pmax(pmin(zmat, hi), lo)
  col_fun <- circlize::colorRamp2(c(lo, hi), c("white", "#cb181d"))
  list(mat = zmat, col_fun = col_fun, limits = c(lo, hi))
}

# Apply clipping and construct the lower (numeric) heatmap with a compact height
s <- auto_diverging(bot_mat, q_neg = 0.05, q_pos = 0.95)
bot_mat <- s$mat
col_fun <- s$col_fun

hm_top <- Heatmap(
  bot_mat, name = "Z-score",
  col = col_fun,
  cluster_rows = FALSE, cluster_columns = FALSE,
  show_row_names = TRUE,  row_names_side = "left",
  show_column_names = FALSE,
  height = unit(22, "mm"),
  heatmap_legend_param = list(title = "Z-score"))

# Stack the numeric block (top) above the annotation canvas
ht_all <-  hm_top %v% ht_2

pdf(file.path(out_dir, "Figure_3B_analog.pdf"), width = 12, height = 6)
draw(ht_all, heatmap_legend_side = "right", annotation_legend_side = "right")
dev.off()

```

```{r}

```
